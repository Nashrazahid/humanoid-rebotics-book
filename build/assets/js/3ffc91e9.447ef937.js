"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[653],{7588:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>c});var s=i(4848),t=i(8453);const r={title:"AI Models in Physical Systems",sidebar_position:4},a="AI Models in Physical Systems",o={id:"module1/chapter4-ai-models-in-physical-systems",title:"AI Models in Physical Systems",description:"Learning Outcomes",source:"@site/docs/module1/chapter4-ai-models-in-physical-systems.md",sourceDirName:"module1",slug:"/module1/chapter4-ai-models-in-physical-systems",permalink:"/humanoid-robotics-book/docs/module1/chapter4-ai-models-in-physical-systems",draft:!1,unlisted:!1,editUrl:"https://github.com/your-username/humanoid-robotics-book/edit/main/docs/module1/chapter4-ai-models-in-physical-systems.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{title:"AI Models in Physical Systems",sidebar_position:4},sidebar:"docs",previous:{title:"Robotics System Components (Sensors, Actuators, Control)",permalink:"/humanoid-robotics-book/docs/module1/chapter3-robotics-system-components"},next:{title:"Mechanical Design Principles",permalink:"/humanoid-robotics-book/docs/module2/chapter4-mechanical-design-principles"}},l={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Digital AI vs. Physical AI Models",id:"digital-ai-vs-physical-ai-models",level:3},{value:"Vision-Language-Action (VLA) Framework",id:"vision-language-action-vla-framework",level:3},{value:"AI Model Architectures for Physical Systems",id:"ai-model-architectures-for-physical-systems",level:3},{value:"Perception Networks",id:"perception-networks",level:4},{value:"Decision Networks",id:"decision-networks",level:4},{value:"Control Networks",id:"control-networks",level:4},{value:"Model Deployment Challenges",id:"model-deployment-challenges",level:3},{value:"Real-time Constraints",id:"real-time-constraints",level:4},{value:"Safety and Reliability",id:"safety-and-reliability",level:4},{value:"Adaptation and Learning",id:"adaptation-and-learning",level:4},{value:"Visuals and Diagrams",id:"visuals-and-diagrams",level:2},{value:"Examples and Demos",id:"examples-and-demos",level:2},{value:"Example 1: End-to-End VLA System",id:"example-1-end-to-end-vla-system",level:3},{value:"Example 2: Multi-step Task Execution",id:"example-2-multi-step-task-execution",level:3},{value:"Theoretical Foundations",id:"theoretical-foundations",level:2},{value:"Grounded Cognition",id:"grounded-cognition",level:3},{value:"Affordance Learning",id:"affordance-learning",level:3},{value:"Interactive Learning",id:"interactive-learning",level:3},{value:"Validation and Testing",id:"validation-and-testing",level:2},{value:"Concept Check: VLA Requirements",id:"concept-check-vla-requirements",level:3},{value:"References",id:"references",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"ai-models-in-physical-systems",children:"AI Models in Physical Systems"}),"\n",(0,s.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,s.jsx)(n.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Distinguish between AI models for digital and physical systems"}),"\n",(0,s.jsx)(n.li,{children:"Understand the Vision-Language-Action (VLA) framework"}),"\n",(0,s.jsx)(n.li,{children:"Describe how AI models interface with robotic control systems"}),"\n",(0,s.jsx)(n.li,{children:"Identify the challenges of deploying AI models on physical robots"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"AI models in physical systems represent a significant departure from traditional digital AI applications. These models must process real-time sensory data, make decisions under uncertainty, and generate actions that affect the physical world. This chapter explores the unique requirements and approaches for AI models in physical systems, with a focus on the emerging Vision-Language-Action (VLA) paradigm that enables robots to understand and respond to natural language commands through physical actions."}),"\n",(0,s.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,s.jsx)(n.h3,{id:"digital-ai-vs-physical-ai-models",children:"Digital AI vs. Physical AI Models"}),"\n",(0,s.jsx)(n.p,{children:"Digital AI models typically:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Process static inputs in batch mode"}),"\n",(0,s.jsx)(n.li,{children:"Generate outputs without physical consequences"}),"\n",(0,s.jsx)(n.li,{children:"Operate in well-defined virtual environments"}),"\n",(0,s.jsx)(n.li,{children:"Have unlimited time for computation"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Physical AI models must:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Process streaming sensory data in real-time"}),"\n",(0,s.jsx)(n.li,{children:"Generate actions with immediate physical consequences"}),"\n",(0,s.jsx)(n.li,{children:"Operate in uncertain, dynamic environments"}),"\n",(0,s.jsx)(n.li,{children:"Meet strict timing constraints for safety and performance"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"vision-language-action-vla-framework",children:"Vision-Language-Action (VLA) Framework"}),"\n",(0,s.jsx)(n.p,{children:"The VLA framework integrates three modalities:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Vision"}),": Processing visual information from cameras and other visual sensors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Language"}),": Understanding natural language commands and context"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action"}),": Generating sequences of physical actions to achieve goals"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This unified approach enables robots to follow complex natural language instructions by perceiving their environment and executing appropriate physical actions."}),"\n",(0,s.jsx)(n.h3,{id:"ai-model-architectures-for-physical-systems",children:"AI Model Architectures for Physical Systems"}),"\n",(0,s.jsx)(n.h4,{id:"perception-networks",children:"Perception Networks"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Convolutional Neural Networks (CNNs) for image processing"}),"\n",(0,s.jsx)(n.li,{children:"Vision Transformers (ViTs) for complex scene understanding"}),"\n",(0,s.jsx)(n.li,{children:"3D Point Cloud Networks for spatial reasoning"}),"\n",(0,s.jsx)(n.li,{children:"Multi-modal fusion networks for sensor integration"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"decision-networks",children:"Decision Networks"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Reinforcement Learning (RL) for policy learning"}),"\n",(0,s.jsx)(n.li,{children:"Planning networks for multi-step reasoning"}),"\n",(0,s.jsx)(n.li,{children:"Language models for instruction understanding"}),"\n",(0,s.jsx)(n.li,{children:"World models for prediction and simulation"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"control-networks",children:"Control Networks"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Model Predictive Control (MPC) for trajectory optimization"}),"\n",(0,s.jsx)(n.li,{children:"Deep neural networks for low-level control"}),"\n",(0,s.jsx)(n.li,{children:"Imitation learning for skill acquisition"}),"\n",(0,s.jsx)(n.li,{children:"Adaptive control for changing conditions"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"model-deployment-challenges",children:"Model Deployment Challenges"}),"\n",(0,s.jsx)(n.h4,{id:"real-time-constraints",children:"Real-time Constraints"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Latency requirements for safe interaction"}),"\n",(0,s.jsx)(n.li,{children:"Computational efficiency for embedded systems"}),"\n",(0,s.jsx)(n.li,{children:"Memory constraints on robotic platforms"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"safety-and-reliability",children:"Safety and Reliability"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Uncertainty quantification for safe decision-making"}),"\n",(0,s.jsx)(n.li,{children:"Fail-safe mechanisms for unexpected situations"}),"\n",(0,s.jsx)(n.li,{children:"Validation and verification of AI decisions"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"adaptation-and-learning",children:"Adaptation and Learning"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Online learning from physical interactions"}),"\n",(0,s.jsx)(n.li,{children:"Transfer learning between simulation and reality"}),"\n",(0,s.jsx)(n.li,{children:"Few-shot adaptation to new environments"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"visuals-and-diagrams",children:"Visuals and Diagrams"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"VLA Architecture:\n[Visual Input] \u2192 [Vision Encoder] \u2192\n                  \u2193\n[Natural Language] \u2192 [Language Encoder] \u2192 [Fusion Layer] \u2192 [Action Decoder] \u2192 [Robot Actions]\n                  \u2193\n              [Task Context]\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"AI Model Integration:\nSensors \u2192 Perception \u2192 AI Models \u2192 Planning \u2192 Control \u2192 Actuators\n         Pipeline    Decision     Layer    Layer\n"})}),"\n",(0,s.jsx)(n.h2,{id:"examples-and-demos",children:"Examples and Demos"}),"\n",(0,s.jsx)(n.h3,{id:"example-1-end-to-end-vla-system",children:"Example 1: End-to-End VLA System"}),"\n",(0,s.jsx)(n.p,{children:'Natural language command: "Please bring me the red cup from the table"'}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Vision"}),": Camera detects objects on table, identifies red cup"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Language"}),': NLP system parses command, identifies "red cup" and "bring" action']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action"}),": Robot plans approach trajectory, grasps cup, navigates to user"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"example-2-multi-step-task-execution",children:"Example 2: Multi-step Task Execution"}),"\n",(0,s.jsx)(n.p,{children:'Command: "Clean the table and put the book in the shelf"'}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Perception"}),": Analyze table to identify items and their categories"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Planning"}),": Sequence of actions to clear table, identify book, locate shelf"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Execution"}),": Execute manipulation and navigation actions sequentially"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"theoretical-foundations",children:"Theoretical Foundations"}),"\n",(0,s.jsx)(n.h3,{id:"grounded-cognition",children:"Grounded Cognition"}),"\n",(0,s.jsx)(n.p,{children:"The theory that cognitive processes are grounded in sensory and motor experiences, which is particularly relevant for physical AI systems."}),"\n",(0,s.jsx)(n.h3,{id:"affordance-learning",children:"Affordance Learning"}),"\n",(0,s.jsx)(n.p,{children:"Learning what actions are possible in different situations based on environmental properties and object characteristics."}),"\n",(0,s.jsx)(n.h3,{id:"interactive-learning",children:"Interactive Learning"}),"\n",(0,s.jsx)(n.p,{children:"Learning through interaction with the environment rather than passive observation, enabling robots to acquire physical understanding."}),"\n",(0,s.jsx)(n.h2,{id:"validation-and-testing",children:"Validation and Testing"}),"\n",(0,s.jsx)(n.h3,{id:"concept-check-vla-requirements",children:"Concept Check: VLA Requirements"}),"\n",(0,s.jsx)(n.p,{children:"Which of the following is NOT a critical requirement for VLA systems?"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Real-time processing capabilities"}),"\n",(0,s.jsx)(n.li,{children:"Multi-modal integration"}),"\n",(0,s.jsx)(n.li,{children:"Batch processing for accuracy"}),"\n",(0,s.jsx)(n.li,{children:"Safety mechanisms"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Answer: 3. Batch processing for accuracy (VLA systems need real-time processing)"}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsx)(n.p,{children:"[1] Chen, K., Zeng, A., Ichter, B., Choromanski, K., Welker, K., Tompson, J., ... & Finn, C. (2023). A Generalist Robot Learning Model via Interactive Scripting. arXiv preprint arXiv:2312.03826."}),"\n",(0,s.jsx)(n.p,{children:"[2] Brohan, A., Brown, J., Carbajal, D. B., Chebotar, Y., Darrin, E., Dayer, R., ... & Vanhoucke, V. (2022). RT-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817."}),"\n",(0,s.jsx)(n.p,{children:"[3] Ahn, M., Brohan, A., Chebotar, Y., Dyer, M., Finn, C., Grover, A., ... & Gu, K. (2022). Do as i can, not as i say: Grounding embodied agents in natural language instructions. arXiv preprint arXiv:2204.01691."}),"\n",(0,s.jsx)(n.p,{children:"[4] Nair, A. V., Martin-Martin, R., Garg, D., Ahn, M., Brohan, A., Brown, J., ... & Guez, A. (2022). Transfusion: Understanding fault-tolerant transformers for robot learning. arXiv preprint arXiv:2206.12984."})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(6540);const t={},r=s.createContext(t);function a(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);