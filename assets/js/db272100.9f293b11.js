"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[963],{6477:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>c,toc:()=>l});var o=i(4848),t=i(8453);const s={title:"Embodiment, Perception, and Action",sidebar_position:2},r="Embodiment, Perception, and Action",c={id:"module1/chapter2-embodiment-perception-action",title:"Embodiment, Perception, and Action",description:"Learning Outcomes",source:"@site/docs/module1/chapter2-embodiment-perception-action.md",sourceDirName:"module1",slug:"/module1/chapter2-embodiment-perception-action",permalink:"/humanoid-rebotics-book/docs/module1/chapter2-embodiment-perception-action",draft:!1,unlisted:!1,editUrl:"https://github.com/nashrazahid/humanoid-rebotics-book/edit/main/docs/module1/chapter2-embodiment-perception-action.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{title:"Embodiment, Perception, and Action",sidebar_position:2},sidebar:"docs",previous:{title:"What is Physical AI",permalink:"/humanoid-rebotics-book/docs/module1/chapter1-what-is-physical-ai"},next:{title:"Robotics System Components (Sensors, Actuators, Control)",permalink:"/humanoid-rebotics-book/docs/module1/chapter3-robotics-system-components"}},a={},l=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Embodiment and Intelligence",id:"embodiment-and-intelligence",level:3},{value:"The Perception-Action Cycle",id:"the-perception-action-cycle",level:3},{value:"Sensor Modalities",id:"sensor-modalities",level:3},{value:"Vision Systems",id:"vision-systems",level:4},{value:"Proprioception",id:"proprioception",level:4},{value:"Exteroception",id:"exteroception",level:4},{value:"Visuals and Diagrams",id:"visuals-and-diagrams",level:2},{value:"Examples and Demos",id:"examples-and-demos",level:2},{value:"Example 1: Object Detection and Grasping",id:"example-1-object-detection-and-grasping",level:3},{value:"Example 2: Navigation with Obstacle Avoidance",id:"example-2-navigation-with-obstacle-avoidance",level:3},{value:"Theoretical Foundations",id:"theoretical-foundations",level:2},{value:"Active Vision Theory",id:"active-vision-theory",level:3},{value:"Sensorimotor Contingency Theory",id:"sensorimotor-contingency-theory",level:3},{value:"Morphological Computation",id:"morphological-computation",level:3},{value:"Validation and Testing",id:"validation-and-testing",level:2},{value:"Concept Check: Perception-Action Cycle",id:"concept-check-perception-action-cycle",level:3},{value:"References",id:"references",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"embodiment-perception-and-action",children:"Embodiment, Perception, and Action"}),"\n",(0,o.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,o.jsx)(n.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Explain the relationship between embodiment and intelligence"}),"\n",(0,o.jsx)(n.li,{children:"Describe the perception-action cycle in physical AI systems"}),"\n",(0,o.jsx)(n.li,{children:"Identify different types of sensors and their role in perception"}),"\n",(0,o.jsx)(n.li,{children:"Understand how action influences perception in embodied systems"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"The relationship between embodiment, perception, and action forms the foundation of Physical AI. Unlike traditional AI systems that process information in isolation, physical AI systems are deeply intertwined with their environment through continuous perception-action cycles. This chapter explores how embodiment shapes intelligence and how perception and action work together in embodied systems."}),"\n",(0,o.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,o.jsx)(n.h3,{id:"embodiment-and-intelligence",children:"Embodiment and Intelligence"}),"\n",(0,o.jsx)(n.p,{children:"Embodiment is not merely about having a physical form; it's about how the physical form influences cognitive processes. The body acts as both a constraint and an opportunity, shaping how an agent can interact with and understand its environment."}),"\n",(0,o.jsx)(n.h3,{id:"the-perception-action-cycle",children:"The Perception-Action Cycle"}),"\n",(0,o.jsx)(n.p,{children:"Physical AI systems operate in a continuous loop:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Perception"}),": Gathering information from the environment through sensors"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Interpretation"}),": Processing sensory data to understand the current state"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action Planning"}),": Deciding on appropriate actions based on goals and state"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action Execution"}),": Performing physical actions that affect the environment"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Feedback"}),": New sensory information resulting from actions"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"sensor-modalities",children:"Sensor Modalities"}),"\n",(0,o.jsx)(n.p,{children:"Physical AI systems typically employ multiple sensor types:"}),"\n",(0,o.jsx)(n.h4,{id:"vision-systems",children:"Vision Systems"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Cameras for visual information"}),"\n",(0,o.jsx)(n.li,{children:"Depth sensors for 3D perception"}),"\n",(0,o.jsx)(n.li,{children:"Thermal cameras for heat detection"}),"\n",(0,o.jsx)(n.li,{children:"Event-based cameras for fast motion capture"}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"proprioception",children:"Proprioception"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Joint encoders for position feedback"}),"\n",(0,o.jsx)(n.li,{children:"Force/torque sensors for contact information"}),"\n",(0,o.jsx)(n.li,{children:"IMU (Inertial Measurement Unit) for orientation and acceleration"}),"\n",(0,o.jsx)(n.li,{children:"Tactile sensors for contact and texture"}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"exteroception",children:"Exteroception"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"LIDAR for distance measurement"}),"\n",(0,o.jsx)(n.li,{children:"Microphones for auditory information"}),"\n",(0,o.jsx)(n.li,{children:"Chemical sensors for environmental composition"}),"\n",(0,o.jsx)(n.li,{children:"GPS for location (outdoor systems)"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"visuals-and-diagrams",children:"Visuals and Diagrams"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Perception \u2192 Interpretation \u2192 Action Planning \u2192 Action Execution \u2192 Environment\n    \u2191                                                                  \u2193\n    +---------------------- Feedback ----------------------------------+\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Multi-sensor Integration:\n[Visual]     [Tactile]     [Proprioceptive]\n    |            |               |\n    v            v               v\n[Sensor Fusion] \u2192 [World Model] \u2192 [Action Selection]\n"})}),"\n",(0,o.jsx)(n.h2,{id:"examples-and-demos",children:"Examples and Demos"}),"\n",(0,o.jsx)(n.h3,{id:"example-1-object-detection-and-grasping",children:"Example 1: Object Detection and Grasping"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Perception"}),": Camera detects an object on a table"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Interpretation"}),": System identifies object type, position, and orientation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action Planning"}),": Calculates approach trajectory and grasp configuration"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action Execution"}),": Moves arm to grasp the object"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Feedback"}),": Tactile sensors confirm successful grasp"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"example-2-navigation-with-obstacle-avoidance",children:"Example 2: Navigation with Obstacle Avoidance"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Perception"}),": LIDAR detects obstacles in the path"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Interpretation"}),": System maps obstacles in local coordinate frame"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action Planning"}),": Calculates alternative path around obstacles"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action Execution"}),": Moves robot along new path"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Feedback"}),": Continuous sensor updates refine path as needed"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"theoretical-foundations",children:"Theoretical Foundations"}),"\n",(0,o.jsx)(n.h3,{id:"active-vision-theory",children:"Active Vision Theory"}),"\n",(0,o.jsx)(n.p,{children:"Vision is not passive but involves active selection of visual information through eye movements, head movements, and body positioning."}),"\n",(0,o.jsx)(n.h3,{id:"sensorimotor-contingency-theory",children:"Sensorimotor Contingency Theory"}),"\n",(0,o.jsx)(n.p,{children:"Perception is understood through the predictable changes in sensory input that result from specific motor actions."}),"\n",(0,o.jsx)(n.h3,{id:"morphological-computation",children:"Morphological Computation"}),"\n",(0,o.jsx)(n.p,{children:'The idea that part of the "computation" required for intelligent behavior is performed by the physical body itself, reducing the burden on the control system.'}),"\n",(0,o.jsx)(n.h2,{id:"validation-and-testing",children:"Validation and Testing"}),"\n",(0,o.jsx)(n.h3,{id:"concept-check-perception-action-cycle",children:"Concept Check: Perception-Action Cycle"}),"\n",(0,o.jsx)(n.p,{children:"In the perception-action cycle, what happens if the feedback loop is broken?"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"System continues to function normally"}),"\n",(0,o.jsx)(n.li,{children:"System becomes reactive rather than proactive"}),"\n",(0,o.jsx)(n.li,{children:"System cannot adapt to environmental changes"}),"\n",(0,o.jsx)(n.li,{children:"Both 2 and 3"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Answer: 4. Both 2 and 3"}),"\n",(0,o.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,o.jsx)(n.p,{children:"[1] Clark, A. (2008). Supersizing the mind: Embodiment, action, and cognitive extension. Oxford University Press."}),"\n",(0,o.jsx)(n.p,{children:"[2] Hutto, D. D., & Myin, E. (2013). Radicalizing enactivism: Basic minds without content. MIT Press."}),"\n",(0,o.jsx)(n.p,{children:"[3] Pfeifer, R., & Scheier, C. (1999). Understanding intelligence. MIT Press."}),"\n",(0,o.jsx)(n.p,{children:"[4] Ballard, D. H., Hayhoe, M. M., Pook, P. K., & Rao, R. P. (1997). Deictic codes for the embodiment of cognition. Behavioral and Brain Sciences, 20(4), 723-742."})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>c});var o=i(6540);const t={},s=o.createContext(t);function r(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);